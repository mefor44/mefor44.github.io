---
layout: post
title: "From Browsing to Buying: Embedding-Based Recommendations at eBay"
date: 2024-10-01
categories: blog
---

## Introduction
I this post I would like to take a closer look at Ebay's recommdation system based on following paper: [Personalized Embedding-based e-Commerce Recommendations at eBay](https://arxiv.org/abs/2102.06156).

Structure of this post:
1. [Problem formulation](#problem-formulation)
1. [Modelling Approach](#modelling-approach)
1. [Data & Experiments](#data--experiments)
1. [Serving](#serving)
1. [Summary](#summary)


## Problem Formulation
Let's start with problem formulation. Overall goal is to produce personalized recommendations. What does that mean? For a given user provide a list of items that match his taste. Sample of such recommendation is persented below.

<div style="text-align: center;">
  <img src="/resources/personalized-retrieval-ebay/sample-recommendations.png" width="600" />
</div>
We assume that each user has some browsing history that represent his interest in products he interacted with. The pool of items availabe on Ebay is massive - 1.6 billion live items, viewed by over 183 million of users. Moreover the items are sold most often in single quantity, which means that offers live rather short on the platfrom. There are millions of items addded daily so a **cold start** is a significant problem here. As offers quickly dissaper the implicit feedback generated by users is **extremaly sparse**.

The authors propose to solve aforementioned problem with Two-Tower model architecture with one tower for user and one tower for items. Therefore users and items are represented in the same vector space, which allows for efficient retrieval methods for candidate item generation. To solve cold start problem items are represented using **content features only**. Moreover they utilize multi-modal events to characterize a user (like items views and search queries).

Deploying such model in a large scale dynamic environment creates many challenges - they will be discussed in [Serving](#serving) section, alongside ideas like cluser-based KNN for increasing recommendation diversity.

## Modelling Approach
Recommendation problem is modelled as classification problem with softmax probability.

$$
P(s_i | U) = \frac{e^{\gamma(v_i, u)}}{\sum_{j \in V} e^{\gamma(v_j, u)}},
$$

where $$P(s_i | U)$$ is the probability of item $$s_i$$ being relevant to user $$U$$, and:
- $$ u \in \mathbb{R}^D $$ is a $$ D $$-dimensional vector for the embedding of user $$ U$$.
- $$ v_i \in \mathbb{R}^D $$ is a $$ D $$-dimensional vector for the embedding of item $$ s_i$$.
- $$ \gamma $$ is the affinity function between the user and item embeddings.
- $$ V $$ represents all items available in the system.

As it is infeasible to perform full softmax operation (considering number of items), negative sampling is performed to limit the size of $$V$$ (negative sampling is discusses [here](#negative-sampling)).
Whole model is trained to minimize Negative Log-likelihood of observed interaction between users and items.

### Model architecture 

Their model architecture follows two-tower paradigm. One tower is for items, second one for users. See image below.
<div style="text-align: center;">
  <img src="/resources/personalized-retrieval-ebay/model-architecture.png" width="800" />
</div>
Item tower embeds different features available for a given product - title, structured aspects (e.g brand: Apple, network: Verizon, etc.) and category. They use Continuous-Bag-of-Words approach for generating feature representations for title and aspect. For category they just use lookup table to embed different categories into dense high-dimensional space. After passing concatenated embeddings through MLP and normalizing it we obtain final item embedding. Later, at serving stage this tower will be used to precompute the index for ANN ([Approximate Nearest Neighbors](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)) search. 

Second tower is for users. It takes a history of user actions (their consider them to be "multimodal", and take different types of actions, like clicks, product views and query searches), where each action is connected with either particular item or query. For former they just take item embedding (from item tower). For latter they treat a query as "pseudoitem" - compute embedding as it is an item, with title equals to query, category is predicted by another model and aspect feature is left blank. This procedure translates to ~4% improvement in offline Recall@20 metric. After obtaing $$n$$ embeddings for a user, they are either averaged or passed through GRU network to compute final user embedding. It is then combined with candidate item embedding via dot product operation. Using recurrent user representation resulted in a ∼5% gain inRecall@20 metric compared to averaging.

Assuming $$u$$ is a representataion of user $$U$$, $$v_i$$ is a representation of item $$s_i$$ the final score between $$(U, s_i)$$ pair is computed with following afinity function:


$$
\gamma(v_i, u) = \frac{v_i^\top u}{\tau},
$$

where $$\tau$$ is a temperature hyperparameter. This is a basiacally just a scaled dot product. According to the authors adding this scaling factor improved their offline recall by 150%.

## Data & Experiments
Trainig data for this dataset was collected from user logs. Each time a user is shown a listing with items he may click on some itme. If he clicks on a particular item this is considered a positive interaction. Initially all non-clicked (but impressed) items (from the same listing view) were treated as negatives - but this approach failed. Items shown together are generally very similar and model was unable to differentiate positive and negative exmaples based only on content features, thus it wasn't able to generalize well. To collect data for user representations 30 days of onsite activity was used, for collecting positive clicks 8 days of logs were used.
One interesting design choice was to limit each user contribution to maximally one interaction in training and validation data. This was done to prevent biasing the overall outcome towards users with high activity. In their data they gathered $$10M$$ and $$104K$$ unique users for train and validation data, respectively.

### Negative sampling
Due to the fact that using "natural" negatives failed, authors needed other way of selecting negative instances. They resorted to a variant of a technique called *in-batch negative sampling*. How does it work? In general this method work as follows. For each positive item from a batch you randomly sample negatives from other items from the same batch. This can lead to false negatives, but when the pool of items is sufficiently large this shouldn't be an issue. Moreover this way of sampling connected to likelihhod based sampling - each item has a probability of being selected proportional to number of times it appears in training data. One differece used in Ebay was that they sampled negatives from uncliked (but observed) items from other items from a batch. Let's consider simple example. Batch size of $$2$$: ($$i_1$$, $$[i_2, i_3, i_4, i_5]$$), ($$i_7$$, $$[i_9, i_{17}, i_{21}, i_{44}]$$), 

where:
- $$i_1$$, $$i_7$$ - positive (clicked) items, 
- $$[i_2, i_3, i_4, i_5]$$ - viewed but unclicked items (in a session when $$i_1$$ was clicked)
- $$[i_9, i_{17}, i_{21}, i_{44}]$$ - viewed but unclicked items (in a session when $$i_7$$ was clicked)

To select a negative for item $$i_1$$ just sample one item from $$[i_9, i_{17}, i_{21}, i_{44}]$$. When batch size is more than $$2$$, there will be more candidates to sample from.

### Experiments
#### Offline evaluation
As a evaluation metric they selected $$Recall@K$$. For training details (hyperparameters) consider looking into [original paper](https://arxiv.org/abs/2102.06156).

As a baseline method they selected Recently Viewed Items (RVI) algorithm. It recommends items that a user has recently viewed ranked by the viewed item’s recency. It is a very simple but strong baseline (if you are want to read more about see [here](https://arxiv.org/abs/1709.07545) and [here](https://dl.acm.org/doi/10.1145/2911451.2914726)). Resuls are presented in table below.

<div style="text-align: center;">
  <img src="/resources/personalized-retrieval-ebay/offline-evaluation.png" width="350" />
</div>
They managed to outperform baseline on different level of $$k$$ (this evaluation was performed on separate test set).

#### Ablation study
Rarerly done, authors investigated impact of missing some parts of users history. Idea: what happens in prudction when some part of user history is missing? More precisely - most recent one. Plot below summarizes what happes with models trained under either whole data, or data with dropped most recent user actions. Evaluation is performed on validation set, but some part of user history is dropped.

<div style="text-align: center;">
  <img src="/resources/personalized-retrieval-ebay/ablation-study.png" width="500" />
</div>
<p></p>
On y-axis there is performance metric - $$Recall@20$$. On x-axis we see how much of user history was skipped when doing predictions. Each line refers to one model, trainied under different setup (with varying amount of data dropped from user history).

<p></p>
For model trained with whole data results degrade very quickly. Dropping some obeservations (even as little as 10 minuts or 1 hour) makes model significantly more robust. On the other hand there is a degradation in metrics when dropping most recent observations - for Ebay the production model was selected as a one with highest area under curve (green line). 


## Serving
Although all tech stack mentioned in the paper allows for online inference, it is still done in offline manner.

### Architecture

The architecture, illustrated in figure below, updates user embeddings and ANN results daily based on constantly changing user browsing history. Since the model relies on mostly static content-based features, it doesn't require daily full model retraining.

<div style="text-align:center;">
  <img src="/resources/personalized-retrieval-ebay/serving-architecture.png" width="350" />
</div>

The prediction process is performed offline in batch mode, utilizing two Spark ETL jobs to generate candidate items and updated user histories from the last 30 days, aggregating necessary metadata in Hadoop. Candidate items are limited to those with two or more clicks in the past four days to control for popularity.

The item and user embeddings are generated using eBay’s GPU cluster, Krylov, through a forward pass in the trained model. A KNN search is then performed on the item embeddings, with results (candidate items for users) stored in a Couchbase database for fast retrieval. This caching system supports hundreds of millions of users and is optimized for low latency.

The backend application, written in Scala and running on the Java Virtual Machine (JVM), meets the performance requirements for serving recommendations. Unlike traditional in-memory matrix factorization methods, this architecture is scalable for eBay's data volume. Additionally, it can apply a separately trained Learning-To-Rank (LTR) model to enhance conversion optimization.

### Clustering to increase diversity

There are many similar items on Ebay site. As metioned in the paper:
<div style="text-align:center;">
<i>at the time of paper writing, searching “iphone 11” on eBay would return 3047 results. As our model only consumes content-based features (title, category, and aspects) for items, all those content-similar items would have similar embedding from our model. Utilizing the traditional KNN approach, given a user embedding, the retrieved items would be extremely overlapping in the embedding space.</i>
</div>
<p></p>

Final retrieved recommedations suffer from low diversity. To overcome this problem authors clustered their product index into $$100\ 000$$ clusters, each with centroid $$c_i$$. They query the index to find $$M$$ nearest clusters and then in each cluster they look for some number of items (for more details and formulas see [original paper](https://arxiv.org/abs/2102.06156)). Results of *no-clustering* vs *clustering* can be seen on image below.

<div style="text-align: center;">
  <img src="/resources/personalized-retrieval-ebay/clusters-diversity.png" width="600" />
</div>
What clustering essentialy does it creates "pseudo-catalog". Items with similar content are organise together in the same clusters. You can balance diversity and retrieval metrics with balancing $$M$$ parameter (with $$M=1$$ this idea degenerates to standard KNN).

### Online evaluation 
From A/B tests, a new model demonstrated a 6% increase in relevant metric compared to previous model running on production, and was deplyoed to production. 


## Summary

In this post we looked into Ebay's production recommender system, that is able to serve personalized recommendations to millions of users. Some key ideas to remember:

* addressing cold start problem with content-based embeddings
* utilizing various implicit signals to model users (items views, search queries)
* model architecture that allows for efficient serving
* increasing recommendations diversity by incorporating clustering layer into KNN search
* model robustness with user data ablation
* deployment at scale
