import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import datetime as dt
import matplotlib.pyplot as plt
import numpy as np
from collections import defaultdict
from tqdm import tqdm
import random

seed = 1234
random.seed(seed)
np.random.seed(seed)
torch.use_deterministic_algorithms(mode=True)
gen = torch.Generator()
gen.manual_seed(seed)


import zipfile
import pandas as pd
with zipfile.ZipFile("./../data/ml-latest-small.zip") as z:
    with z.open("ml-latest-small/ratings.csv") as f:
        ratings = pd.read_csv(f, delimiter=",")


ratings.head()


ratings['datetime'] = ratings['timestamp'].apply(lambda x: dt.datetime.fromtimestamp(x / 1000.0))
ratings["date"] = pd.to_datetime(ratings['datetime']).dt.date
interactions = ratings[ratings.rating > 3]
ratings.shape, interactions.shape


interactions.userId.nunique(), interactions.movieId.nunique()


n_interactions = interactions.groupby("date").agg(n_obs=("userId","count")).reset_index()
n_interactions
plt.bar(n_interactions.date, n_interactions.n_obs)
plt.title("Number of interactions in given day")
plt.ylabel("n interactions")
plt.xticks(rotation=45)
plt.show()


split_date = "1970-01-18"
train = interactions[interactions.datetime < split_date]
test = interactions[interactions.datetime >= split_date]
train.datetime.max(), len(train), test.datetime.min(), len(test)


users, items = train.userId.unique(), train.movieId.unique()
n_users, n_items = len(users), len(items)
users_df = pd.DataFrame(users, columns=["userId"])
items_df = pd.DataFrame(items, columns=["movieId"])
u_to_ids = {user:idx for idx,user in enumerate(users)}
i_to_ids = {item:idx for idx,item in enumerate(items)}
train["users_mapped"] = train["userId"].apply(lambda x: u_to_ids[x])
train["items_mapped"] = train["movieId"].apply(lambda x: i_to_ids[x])
n_users, n_items


print(f"Test n obs before merge = {test.shape[0]}")
test_valid = test.merge(users_df, on="userId").merge(items_df, on="movieId")
print(f"Test n obs after merge = {test_valid.shape[0]}")


class MF(nn.Module):
    def __init__(self, n_factors=None):
        pass
    
    def forward(self, user, item):
        pass
        


class MatrixFactorization(nn.Module):
    def __init__(self, n_factors=None, n_users=None, n_items=None):
        super().__init__()
        self.n_factors = n_factors
        self.users = nn.Embedding(n_users, n_factors)
        self.items = nn.Embedding(n_items, n_factors)
    
    def forward(self, user, item):
        user_emb, item_emb = self.users(user), self.items(item)
        return (user_emb * item_emb).sum(dim=1)


def recall_k(k=10, annotated_preds=[]):
    pass
    # k - cutoff value
    # preds - list with tuples (probability, 0/1) indicating predicted scores and label


test_positives = test_valid.groupby("userId")["movieId"].agg(lambda x: x.tolist())

items_set = set(items)
positives_test = {}
negatives_test = {}
for u,i_pos in zip(test_positives.index, test_positives):
    sampled = np.random.choice(list(items_set - set(i_pos)), size=100, replace=False)
    positives_test[u] = i_pos
    negatives_test[u] = sampled
    
# encode users and items for model
positives_test_encoded = {}
negatives_test_encoded = {}
for u,itms in positives_test.items():
    positives_test_encoded[u_to_ids[u]] = [i_to_ids[i] for i in itms]
for u,itms in negatives_test.items():
    negatives_test_encoded[u_to_ids[u]] = [i_to_ids[i] for i in itms]


# score positives and negatives
def score(model, positives_test_encoded, negatives_test_encoded):
    scored = defaultdict(list)
    for u, itms in positives_test_encoded.items():
        u_tensor = torch.tensor([u]).repeat(len(itms))
        itms = torch.tensor(itms)
        # evaluate model:
        model.eval()
        with torch.no_grad():
            preds = model(u_tensor, itms)
        for p in preds:
            scored[u].append((p.item(),1))

    for u, itms in negatives_test_encoded.items():
        u_tensor = torch.tensor([u]).repeat(len(itms))
        itms = torch.tensor(itms)
        # evaluate model:
        model.eval()
        with torch.no_grad():
            preds = model(u_tensor, itms)
        for p in preds:
            scored[u].append((p.item(),0))
    return scored


def recall_k(k=10, annotated_preds=[]):
    # k - cutoff value
    # preds - list with tuples (probability, 0/1) indicating predicted scores and label
    n = min(k, sum([i[1] for i in annotated_preds]))
    s = sorted(annotated_preds, key=lambda x: x[0], reverse=True)
    r = sum([i[1] for i in s[:k]])
    return r / n


# init random model
model = MatrixFactorization(n_factors=20, n_users=n_users, n_items=n_items)
scored = score(model, positives_test_encoded, negatives_test_encoded)
for k in [5,10, 30]:
    recalls = [recall_k(k, pred) for pred in scored.values()]
    mean_recall = np.mean(recalls)
    print(f"Recall@{k} = {mean_recall}")


# init random model
model = MatrixFactorization(n_factors=20, n_users=n_users, n_items=n_items)
scored = score(model, positives_test_encoded, negatives_test_encoded)
for k in [5,10, 30]:
    recalls = [recall_k(k, pred) for pred in scored.values()]
    mean_recall = np.mean(recalls)
    print(f"Recall@{k} = {mean_recall}")


# helper function, we will use it later during trainig
def score_recall(model, positives_test_encoded, negatives_test_encoded, k):
    scored = score(model, positives_test_encoded, negatives_test_encoded)
    recalls = [recall_k(k, pred) for pred in scored.values()]
    return np.mean(recalls)


lr = 1e-3
batch_size = 64
n_epochs = 40


model = MatrixFactorization(n_factors=20, n_users=n_users, n_items=n_items)

optimizer = torch.optim.Adam(model.parameters(), lr=lr)
loss_function = nn.BCEWithLogitsLoss()


class MovieLensDataset(Dataset):
    def __init__(self, data, output_col="output"):
        self.users = data.users_mapped.values
        self.items = data.items_mapped.values
        if output_col:
            self.output = data[output_col].values
        else:
            self.output = np.ones(self.users.shape)
        
    def __len__(self):
        return len(self.users)
        
    def __getitem__(self, idx):
        return (self.users[idx], self.items[idx], self.output[idx])


dataset = MovieLensDataset(train, output_col=None)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=gen)


def train_fn(model, optimizer, n_epochs, dataloader, verbose_n_steps=None, eval_recall=True, k=10):
    epoch_losses, recalls = [], []
    global_step = 0
    epoch_loss = []
    for epoch in range(1, n_epochs+1):
        print(f"Running epoch {epoch}")
        tmp_loss = []
        for user, item, output in tqdm(dataloader):
            optimizer.zero_grad()

            # Predict and calculate loss
            prediction = model(user, item)
            loss = loss_function(prediction, output)

            # Backpropagate
            loss.backward()

            # Update the parameters
            optimizer.step()

            global_step += 1
            epoch_loss.append(loss.item())
            
            if verbose_n_steps:
                # store current loss
                tmp_loss.append(loss.item())
            if verbose_n_steps and global_step % verbose_n_steps == 0:
                avg = np.mean(tmp_loss)
                print(f"Step = {global_step}, moving average loss = {avg}")
                tmp_loss = []
                
        e_loss = np.mean(epoch_loss)
        epoch_losses.append(e_loss)
        print(f"Avg loss after epoch {epoch}, is equal to {e_loss}")
        
        if eval_recall:
            r = score_recall(model, positives_test_encoded, negatives_test_encoded, k)
            print(f"Recall@{k} (test set) after epoch {epoch}, is equal to {r}")
            recalls.append(r)
            
    return epoch_losses, recalls


losses, recalls = train_fn(model, optimizer, n_epochs, dataloader, verbose_n_steps=None)


# plot training loss
def plot_vector(vector, title=None, xlabel="epoch", ylabel="loss", figsize=(12,5)):
    plt.figure(figsize=figsize)
    plt.plot([i+1 for i in range(len(vector))],  vector, "o")
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.xticks([i+1 for i in range(len(vector))])
    plt.show()

plot_vector(losses, ylabel="loss", figsize=(14,5))


# plot recall@10 on test set
plot_vector(recalls, ylabel="Recall@10", figsize=(14,5))


scored = score(model, positives_test_encoded, negatives_test_encoded)
for k in [5,10, 30]:
    recalls = [recall_k(k, pred) for pred in scored.values()]
    mean_recall = np.mean(recalls)
    print(f"Recall@{k} = {mean_recall}")


n_negatives = train.shape[0]
users_sampled = np.random.choice(users, n_negatives)
items_sampled = np.random.choice(items, n_negatives)
negative_df = pd.DataFrame({"userId":users_sampled, "movieId":items_sampled, "output":np.zeros(n_negatives)})


train_sampled = pd.concat([train[["userId", "movieId"]], negative_df])
train_sampled = train_sampled.fillna(1)
train_sampled["users_mapped"] = train_sampled["userId"].apply(lambda x: u_to_ids[x])
train_sampled["items_mapped"] = train_sampled["movieId"].apply(lambda x: i_to_ids[x])


dataset_sampled = MovieLensDataset(train_sampled)
dataloader_sampled = DataLoader(dataset_sampled, batch_size=batch_size, shuffle=True, generator=gem)


model_negatives = MatrixFactorization(n_factors=20, n_users=n_users, n_items=n_items)

optimizer = torch.optim.Adam(model_negatives.parameters(), lr=lr)
loss_function = nn.BCEWithLogitsLoss()


losses, recalls = train_fn(model_negatives, optimizer, n_epochs, dataloader=dataloader_sampled)


# plot loss on train set
plot_vector(losses, ylabel="loss", figsize=(14,5))


# plot recall@10 on test set
plot_vector(recalls, ylabel="Recall@10", figsize=(14,5))


scored = score(model_negatives, positives_test_encoded, negatives_test_encoded)
for k in [5,10, 30]:
    recalls = [recall_k(k, pred) for pred in scored.values()]
    mean_recall = np.mean(recalls)
    print(f"Recall@{k} = {mean_recall}")






